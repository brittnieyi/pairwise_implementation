{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2020 Google LLC.\n\n\nLicensed under the Apache License, Version 2.0 (the 'License');\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an 'AS IS' BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.","metadata":{"id":"sK-RhlsGxwpd"}},{"cell_type":"markdown","source":"This colab contains TensorFlow code for implementing the constrained optimization methods presented in the paper:\n> Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, Serena Wang, 'Pairwise Fairness for Ranking and Regression', AAAI 2020. [<a href='https://arxiv.org/pdf/1906.05330.pdf'>link</a>]\n\nFirst, let's install and import the relevant libraries.","metadata":{"id":"fuN9k6Gux-yD"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport sys\nfrom sklearn import model_selection\nimport tensorflow as tf","metadata":{"id":"JXgLyAJm0UyB","execution":{"iopub.status.busy":"2022-11-07T00:41:43.122017Z","iopub.execute_input":"2022-11-07T00:41:43.122411Z","iopub.status.idle":"2022-11-07T00:41:50.192330Z","shell.execute_reply.started":"2022-11-07T00:41:43.122341Z","shell.execute_reply":"2022-11-07T00:41:50.191249Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"We will need the TensorFlow Constrained Optimization (TFCO) library.","metadata":{"id":"iTUyZk_A0XnF"}},{"cell_type":"code","source":"!pip install git+https://github.com/google-research/tensorflow_constrained_optimization","metadata":{"id":"DvhGP5TW0V_J","execution":{"iopub.status.busy":"2022-11-07T00:41:50.194032Z","iopub.execute_input":"2022-11-07T00:41:50.198322Z","iopub.status.idle":"2022-11-07T00:42:24.510982Z","shell.execute_reply.started":"2022-11-07T00:41:50.198275Z","shell.execute_reply":"2022-11-07T00:42:24.508994Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/google-research/tensorflow_constrained_optimization\n  Cloning https://github.com/google-research/tensorflow_constrained_optimization to /tmp/pip-req-build-tjndfxjp\n  Running command git clone --filter=blob:none --quiet https://github.com/google-research/tensorflow_constrained_optimization /tmp/pip-req-build-tjndfxjp\n  Resolved https://github.com/google-research/tensorflow_constrained_optimization to commit 723d63f8567aaa988c4ce4761152beee2b462e1d\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tfco-nightly==0.3.dev20221107) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from tfco-nightly==0.3.dev20221107) (1.7.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tfco-nightly==0.3.dev20221107) (1.15.0)\nRequirement already satisfied: tensorflow>=1.14 in /opt/conda/lib/python3.7/site-packages (from tfco-nightly==0.3.dev20221107) (2.6.4)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.12)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.15.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.3.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.43.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.1.0)\nCollecting numpy\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.6.0)\nCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.2.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.1.2)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (5.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.4.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.37.1)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.19.4)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.12.1)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.6.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.6.3)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.5.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.3.7)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.4.6)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.28.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.2.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.35.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (4.12.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (1.26.11)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=1.14->tfco-nightly==0.3.dev20221107) (3.2.0)\nBuilding wheels for collected packages: tfco-nightly\n  Building wheel for tfco-nightly (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tfco-nightly: filename=tfco_nightly-0.3.dev20221107-py3-none-any.whl size=199270 sha256=89f05b41d37ed4b6dd1ce1112720931e01cb71fe6a59e63b83e8bc502d66d80c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-peuwo2_n/wheels/11/5a/d7/8ec9d3fc460f26c180d5562f321ec044faebefeb9ec101a8cc\nSuccessfully built tfco-nightly\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard, tfco-nightly\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Uninstalling typing_extensions-4.3.0:\n      Successfully uninstalled typing_extensions-4.3.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.0\n    Uninstalling tensorboard-2.10.0:\n      Successfully uninstalled tensorboard-2.10.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\npytorch-lightning 1.7.2 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.2 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nnnabla 1.29.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\njax 0.3.16 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible.\nflax 0.6.0 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.12.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ncmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\nallennlp 2.10.0 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.0 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.3.4 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.27.56 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 tfco-nightly-0.3.dev20221107 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow_constrained_optimization as tfco","metadata":{"id":"iYxlPVVtzWs3","execution":{"iopub.status.busy":"2022-11-07T00:42:24.512909Z","iopub.execute_input":"2022-11-07T00:42:24.513350Z","iopub.status.idle":"2022-11-07T00:42:25.874898Z","shell.execute_reply.started":"2022-11-07T00:42:24.513322Z","shell.execute_reply":"2022-11-07T00:42:25.874064Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Pairwise Ranking Fairness","metadata":{"id":"gBUr48pLzsqK"}},{"cell_type":"markdown","source":"We will be training a linear ranking model $f(x) = w^\\top x$ where $x \\in \\mathbb{R}^d$ is a set of features for a query-document pair. Our goal is to train the model such that it accurately ranks the positive documents in a query above the negative ones.\n","metadata":{"id":"_Wgq7N73rPHV"}},{"cell_type":"markdown","source":"Specifically, for the ranking model $f$, we denote:\n- $err(f)$ as the pairwise ranking error for model $f$ over all pairs of positive and negative documents\n$$\nerr(f) = \\mathbf{E}\\big[\\mathbb{I}\\big(f(x) < f(x')\\big) \\,\\big|\\, y = 1,~ y' = 0\\big]\n$$\n\n\n- $err_{i,j}(f)$ as the pairwise ranking error over positive-negative document pairs where the pos. document is from group $i$, and the neg. document is from group $j$.\n\n$$\nerr_{i, j}(f) = \\mathbf{E}\\big[\\mathbb{I}\\big(f(x) < f(x')\\big) \\,\\big|\\, y = 1,~ y' = 0,~ grp(x) = i, ~grp(x') = j\\big]\n$$\n<br>\n\nWe then wish to solve the following constrained problem:\n$$min_f\\; err(f)$$\n$$\\text{   s.t.   } |err_{i,j}(f) - err_{k,\\ell}(f)| \\leq \\epsilon \\;\\;\\; \\forall ((i,j), (k,\\ell)) \\in \\mathcal{G},$$\n\nwhere $\\mathcal{G}$ contains the pairs we are interested in constraining.","metadata":{"id":"6tZrx9BfOB_Q"}},{"cell_type":"markdown","source":"## Load Communities & Crime Data\n\nWe will use the benchmark Communities and Crimes dataset from the UCI Machine Learning repository for our illustration. This dataset contains various demographic and racial distribution details (aggregated from census and law enforcement data sources) about different communities in the US, along with the per capita crime rate in each commmunity. As is commonly done in the literature, we will bin the crime rate attribute into two categories: \"low crime\" and \"high crime\", and formulate the task of *ranking* the communities such that the high crime ones are above the low crime ones. We consider communities where the percentage of black population is above the 70-th percentile as the protected group.\n","metadata":{"id":"qM-PzAuykOmN"}},{"cell_type":"code","source":"# We will divide the data into 10 batches, and treat each of them as a query.\nnum_queries = 10\n\n# List of column names in the dataset.\ncolumn_names = [\"state\", \"county\", \"community\", \"communityname\", \"fold\", \"population\", \"householdsize\", \"racepctblack\", \"racePctWhite\", \"racePctAsian\", \"racePctHisp\", \"agePct12t21\", \"agePct12t29\", \"agePct16t24\", \"agePct65up\", \"numbUrban\", \"pctUrban\", \"medIncome\", \"pctWWage\", \"pctWFarmSelf\", \"pctWInvInc\", \"pctWSocSec\", \"pctWPubAsst\", \"pctWRetire\", \"medFamInc\", \"perCapInc\", \"whitePerCap\", \"blackPerCap\", \"indianPerCap\", \"AsianPerCap\", \"OtherPerCap\", \"HispPerCap\", \"NumUnderPov\", \"PctPopUnderPov\", \"PctLess9thGrade\", \"PctNotHSGrad\", \"PctBSorMore\", \"PctUnemployed\", \"PctEmploy\", \"PctEmplManu\", \"PctEmplProfServ\", \"PctOccupManu\", \"PctOccupMgmtProf\", \"MalePctDivorce\", \"MalePctNevMarr\", \"FemalePctDiv\", \"TotalPctDiv\", \"PersPerFam\", \"PctFam2Par\", \"PctKids2Par\", \"PctYoungKids2Par\", \"PctTeen2Par\", \"PctWorkMomYoungKids\", \"PctWorkMom\", \"NumIlleg\", \"PctIlleg\", \"NumImmig\", \"PctImmigRecent\", \"PctImmigRec5\", \"PctImmigRec8\", \"PctImmigRec10\", \"PctRecentImmig\", \"PctRecImmig5\", \"PctRecImmig8\", \"PctRecImmig10\", \"PctSpeakEnglOnly\", \"PctNotSpeakEnglWell\", \"PctLargHouseFam\", \"PctLargHouseOccup\", \"PersPerOccupHous\", \"PersPerOwnOccHous\", \"PersPerRentOccHous\", \"PctPersOwnOccup\", \"PctPersDenseHous\", \"PctHousLess3BR\", \"MedNumBR\", \"HousVacant\", \"PctHousOccup\", \"PctHousOwnOcc\", \"PctVacantBoarded\", \"PctVacMore6Mos\", \"MedYrHousBuilt\", \"PctHousNoPhone\", \"PctWOFullPlumb\", \"OwnOccLowQuart\", \"OwnOccMedVal\", \"OwnOccHiQuart\", \"RentLowQ\", \"RentMedian\", \"RentHighQ\", \"MedRent\", \"MedRentPctHousInc\", \"MedOwnCostPctInc\", \"MedOwnCostPctIncNoMtg\", \"NumInShelters\", \"NumStreet\", \"PctForeignBorn\", \"PctBornSameState\", \"PctSameHouse85\", \"PctSameCity85\", \"PctSameState85\", \"LemasSwornFT\", \"LemasSwFTPerPop\", \"LemasSwFTFieldOps\", \"LemasSwFTFieldPerPop\", \"LemasTotalReq\", \"LemasTotReqPerPop\", \"PolicReqPerOffic\", \"PolicPerPop\", \"RacialMatchCommPol\", \"PctPolicWhite\", \"PctPolicBlack\", \"PctPolicHisp\", \"PctPolicAsian\", \"PctPolicMinor\", \"OfficAssgnDrugUnits\", \"NumKindsDrugsSeiz\", \"PolicAveOTWorked\", \"LandArea\", \"PopDens\", \"PctUsePubTrans\", \"PolicCars\", \"PolicOperBudg\", \"LemasPctPolicOnPatr\", \"LemasGangUnitDeploy\", \"LemasPctOfficDrugUn\", \"PolicBudgPerPop\", \"ViolentCrimesPerPop\"]\n\ndataset_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\"\n\n# Read dataset from the UCI web repository and assign column names.\ndata_df = pd.read_csv(dataset_url, sep=\",\", names=column_names,\n                      na_values=\"?\")\n\n# Make sure that there are no missing values in the \"ViolentCrimesPerPop\" column.\nassert(not data_df[\"ViolentCrimesPerPop\"].isna().any())\n\n# Binarize the \"ViolentCrimesPerPop\" column and obtain labels.\ncrime_rate_70_percentile = data_df[\"ViolentCrimesPerPop\"].quantile(q=0.7)\nlabels_df = (data_df[\"ViolentCrimesPerPop\"] >= crime_rate_70_percentile)\n\n# Now that we have assigned binary labels, \n# we drop the \"ViolentCrimesPerPop\" column from the data frame.\ndata_df.drop(columns=\"ViolentCrimesPerPop\", inplace=True)\n\n# Group features.\nrace_black_70_percentile = data_df[\"racepctblack\"].quantile(q=0.7)\ngroups_df = (data_df[\"racepctblack\"] >= race_black_70_percentile)\n\n# Drop categorical features.\ndata_df.drop(columns=[\"state\", \"county\", \"community\", \"communityname\", \"fold\"],\n             inplace=True)\n\n# Handle missing features.\nfeature_names = data_df.columns\nfor feature_name in feature_names:  \n    missing_rows = data_df[feature_name].isna()  # Which rows have missing values?\n    if missing_rows.any():  # Check if at least one row has a missing value.\n        data_df[feature_name].fillna(0.0, inplace=True)  # Fill NaN with 0.\n        missing_rows.rename(feature_name + \"_is_missing\", inplace=True)\n        data_df = data_df.join(missing_rows)  # Append boolean \"is_missing\" feature.\n\nlabels = labels_df.values.astype(np.float32)\ngroups = groups_df.values.astype(np.float32)\nfeatures = data_df.values.astype(np.float32)\n\n# Set random seed so that the results are reproducible.\nnp.random.seed(123456)\n\n# We randomly divide the examples into 'num_queries' queries.\nqueries = np.random.randint(0, num_queries, size=features.shape[0])\n\n# Train, vali and test indices.\ntrain_indices, test_indices = model_selection.train_test_split(\n    range(features.shape[0]), test_size=0.4)\n\n# Train features, labels and protected groups.\ntrain_set = {\n  'features': features[train_indices, :],\n  'labels': labels[train_indices],\n  'groups': groups[train_indices],\n  'queries': queries[train_indices],\n  'dimension': features.shape[-1],\n  'num_queries': num_queries\n}\n\n# Test features, labels and protected groups.\ntest_set = {\n  'features': features[test_indices, :],\n  'labels': labels[test_indices],\n  'groups': groups[test_indices],\n  'queries': queries[test_indices],\n  'dimension': features.shape[-1],\n  'num_queries': num_queries\n}","metadata":{"id":"QSUkaGKxBa2M","execution":{"iopub.status.busy":"2022-11-07T00:42:25.877265Z","iopub.execute_input":"2022-11-07T00:42:25.877557Z","iopub.status.idle":"2022-11-07T00:42:26.634153Z","shell.execute_reply.started":"2022-11-07T00:42:25.877533Z","shell.execute_reply":"2022-11-07T00:42:26.632791Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{"id":"JxBMPRJA2wvW"}},{"cell_type":"markdown","source":"We will need functions to convert labeled data into paired data.","metadata":{"id":"q7WpTPkKAga-"}},{"cell_type":"code","source":"def pair_pos_neg_docs(data):\n  # Returns a DataFrame of pairs of positive-negative docs from given DataFrame.\n  # Separate pos and neg docs.\n  pos_docs = data[data.label == 1]\n  if pos_docs.empty:\n    return\n  neg_docs = data[data.label == 0]\n  if neg_docs.empty:\n    return\n\n  # Include a merge key.\n  pos_docs.insert(0, 'merge_key', 0)\n  neg_docs.insert(0, 'merge_key', 0)\n\n  # Merge docs and drop merge key column.\n  pairs = pos_docs.merge(neg_docs, on='merge_key', how='outer',\n                         suffixes=('_pos', '_neg'))\n  pairs.drop(columns=['merge_key'], inplace=True)\n  return pairs\n\n\ndef convert_labeled_to_paired_data(data_dict, index=None):\n  # Forms pairs of examples from each batch/query.\n\n  # Converts data arrays to pandas DataFrame with required column names and\n  # makes a call to convert_df_to_pairs and returns a dictionary.\n  features = data_dict['features']\n  labels = data_dict['labels']\n  groups = data_dict['groups']\n  queries = data_dict['queries']\n\n  if index is not None:\n    data_df = pd.DataFrame(features[queries == index, :])\n    data_df = data_df.assign(label=pd.DataFrame(labels[queries == index]))\n    data_df = data_df.assign(group=pd.DataFrame(groups[queries == index]))\n    data_df = data_df.assign(query_id=pd.DataFrame(queries[queries == index]))\n  else:\n    data_df = pd.DataFrame(features)\n    data_df = data_df.assign(label=pd.DataFrame(labels))\n    data_df = data_df.assign(group=pd.DataFrame(groups))\n    data_df = data_df.assign(query_id=pd.DataFrame(queries))\n\n  # Forms pairs of positive-negative docs for each query in given DataFrame\n  # if the DataFrame has a query_id column. Otherise forms pairs from all rows\n  # of the DataFrame.\n  data_pairs = data_df.groupby('query_id').apply(pair_pos_neg_docs)\n\n  # Create groups ndarray.\n  pos_groups = data_pairs['group_pos'].values.reshape(-1, 1)\n  neg_groups = data_pairs['group_neg'].values.reshape(-1, 1)\n  group_pairs = np.concatenate((pos_groups, neg_groups), axis=1)\n\n  # Create queries ndarray.\n  queries = data_pairs['query_id_pos'].values.reshape(-1,)\n\n  # Create features ndarray.\n  feature_names = data_df.columns\n  feature_names = feature_names.drop(['query_id', 'label'])\n  feature_names = feature_names.drop(['group'])\n\n  pos_features = data_pairs[[str(s) + '_pos' for s in feature_names]].values\n  pos_features = pos_features.reshape(-1, 1, len(feature_names))\n\n  neg_features = data_pairs[[str(s) + '_neg' for s in feature_names]].values\n  neg_features = neg_features.reshape(-1, 1, len(feature_names))\n\n  features_pairs = np.concatenate((pos_features, neg_features), axis=1)\n\n  # Paired data dict.\n  paired_data = {\n      'features': features_pairs, \n      'groups': group_pairs, \n      'queries': queries,\n      'dimension': data_dict['dimension'],\n      'num_queries': data_dict['num_queries']\n  }\n\n  return paired_data","metadata":{"id":"u0zUW2wEYMes","execution":{"iopub.status.busy":"2022-11-07T00:42:26.635938Z","iopub.execute_input":"2022-11-07T00:42:26.636512Z","iopub.status.idle":"2022-11-07T00:42:26.651891Z","shell.execute_reply.started":"2022-11-07T00:42:26.636486Z","shell.execute_reply":"2022-11-07T00:42:26.650676Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We will also need functions to evaluate the pairwise error rates for a linear model.","metadata":{"id":"H4HV7a7wq7rm"}},{"cell_type":"code","source":"def get_mask(groups, pos_group, neg_group=None):\n  # Returns a boolean mask selecting positive-negative document pairs where \n  # the protected group for  the positive document is pos_group and \n  # the protected group for the negative document (if specified) is neg_group.\n  # Repeat group membership positive docs as many times as negative docs.\n  mask_pos = groups[:, 0] == pos_group\n  \n  if neg_group is None:\n    return mask_pos\n  else:\n    mask_neg = groups[:, 1] == neg_group\n    return mask_pos & mask_neg\n\n\ndef error_rate(model, dataset):\n  # Returns error rate for Keras model on dataset.\n  d = dataset['dimension']\n  scores0 = model.predict(dataset['features'][:, 0, 0:d].reshape(-1, d))\n  scores1 = model.predict(dataset['features'][:, 1, 0:d].reshape(-1, d))\n  diff = scores0 - scores1  \n  return np.mean(diff.reshape((-1)) < 0)\n\n\ndef group_error_rate(model, dataset, pos_group, neg_group=None):\n  # Returns error rate for Keras model on data set, considering only document \n  # pairs where the protected group for the positive document is pos_group, and  \n  # the protected group for the negative document (if specified) is neg_group.\n  d = dataset['dimension']\n  scores0 = model.predict(dataset['features'][:, 0, :].reshape(-1, d))\n  scores1 = model.predict(dataset['features'][:, 1, :].reshape(-1, d))\n  mask = get_mask(dataset['groups'], pos_group, neg_group)\n  diff = scores0 - scores1\n  diff = diff[mask > 0].reshape((-1))\n  return np.mean(diff < 0)","metadata":{"id":"K8OQ4ado20p-","execution":{"iopub.status.busy":"2022-11-07T00:42:26.653176Z","iopub.execute_input":"2022-11-07T00:42:26.653572Z","iopub.status.idle":"2022-11-07T00:42:26.691229Z","shell.execute_reply.started":"2022-11-07T00:42:26.653547Z","shell.execute_reply":"2022-11-07T00:42:26.689957Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Create Linear Model\n\n\n","metadata":{"id":"kI8xNJDcpQYP"}},{"cell_type":"markdown","source":"We then write a function to create the linear ranking model.","metadata":{"id":"lY4hvJAOra6s"}},{"cell_type":"code","source":"def create_ranking_model(features, dimension):\n  # Returns a linear Keras ranking model, and returns a nullary function \n  # returning predictions on the features.\n\n  # Linear ranking model with no hidden layers.\n  # No bias included as this is a ranking problem.\n  layers = []\n  # Input layer takes `dimension` inputs.\n  layers.append(tf.keras.Input(shape=(dimension,)))\n  layers.append(tf.keras.layers.Dense(1, use_bias=False)) \n  ranking_model = tf.keras.Sequential(layers)\n\n  # Create a nullary function that returns applies the linear model to the \n  # features and returns the tensor with the predictions.\n  def predictions():\n    scores0 = ranking_model(features()[:, 0, :].reshape(-1, dimension))\n    scores1 = ranking_model(features()[:, 1, :].reshape(-1, dimension))\n    return tf.reshape(scores0 - scores1, (-1,))\n\n  return ranking_model, predictions","metadata":{"id":"eTQOebAepXSu","execution":{"iopub.status.busy":"2022-11-07T00:42:26.692808Z","iopub.execute_input":"2022-11-07T00:42:26.694099Z","iopub.status.idle":"2022-11-07T00:42:26.710914Z","shell.execute_reply.started":"2022-11-07T00:42:26.694048Z","shell.execute_reply":"2022-11-07T00:42:26.709974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Formulate Optimization Problem","metadata":{"id":"WIBvG3Arv7zR"}},{"cell_type":"markdown","source":"We are ready to formulate the constrained optimization problem using the TFCO library. ","metadata":{"id":"SfZd-XPt0A8E"}},{"cell_type":"code","source":"def group_mask_fn(groups, pos_group, neg_group=None):\n  # Returns a nullary function returning group mask.\n  group_mask = lambda: np.reshape(\n      get_mask(groups(), pos_group, neg_group), (-1))\n  return group_mask\n\n\ndef formulate_problem(\n    features, groups, dimension, constraint_groups=[], constraint_slack=None):\n  # Formulates a constrained problem that optimizes the error rate for a linear\n  # model on the specified dataset, subject to pairwise fairness constraints \n  # specified by the constraint_groups and the constraint_slack.\n  # \n  # Args:\n  #   features: Nullary function returning features\n  #   groups: Nullary function returning groups\n  #   labels: Nullary function returning labels\n  #   dimension: Input dimension for ranking model\n  #   constraint_groups: List containing tuples of the form \n  #     ((pos_group0, neg_group0), (pos_group1, neg_group1)), specifying the \n  #     group memberships for the document pairs to compare in the constraints.\n  #   constraint_slack: slackness '\\epsilon' allowed in the constraints.\n  # Returns:\n  #   A RateMinimizationProblem object, and a Keras ranking model.\n\n  # Set random seed for reproducibility.\n  random.seed(333333)\n  np.random.seed(121212)\n  tf.random.set_seed(212121)\n\n  # Create linear ranking model: we get back a Keras model and a nullary  \n  # function returning predictions on the features.\n  ranking_model, predictions = create_ranking_model(features, dimension)\n  \n  # Context for the optimization objective.\n  context = tfco.rate_context(predictions)\n  \n  # Constraint set.\n  constraint_set = []\n  \n  # Context for the constraints.\n  for ((pos_group0, neg_group0), (pos_group1, neg_group1)) in constraint_groups:\n    # Context for group 0.\n    group_mask0 = group_mask_fn(groups, pos_group0, neg_group0)\n    context_group0 = context.subset(group_mask0)\n\n    # Context for group 1.\n    group_mask1 = group_mask_fn(groups, pos_group1, neg_group1)\n    context_group1 = context.subset(group_mask1)\n\n    # Add constraints to constraint set.\n    constraint_set.append(\n        tfco.negative_prediction_rate(context_group0) <= (\n            tfco.negative_prediction_rate(context_group1) + constraint_slack))\n    constraint_set.append(\n        tfco.negative_prediction_rate(context_group1) <= (\n            tfco.negative_prediction_rate(context_group0) + constraint_slack))\n  \n  # Formulate constrained minimization problem.\n  problem = tfco.RateMinimizationProblem(\n      tfco.negative_prediction_rate(context), constraint_set)\n  \n  return problem, ranking_model","metadata":{"id":"0AfVknixv9So","execution":{"iopub.status.busy":"2022-11-07T00:42:26.712332Z","iopub.execute_input":"2022-11-07T00:42:26.712562Z","iopub.status.idle":"2022-11-07T00:42:26.732578Z","shell.execute_reply.started":"2022-11-07T00:42:26.712539Z","shell.execute_reply":"2022-11-07T00:42:26.731385Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{"id":"P1x4yEllRKjH"}},{"cell_type":"markdown","source":"The following function then trains the linear model by solving the above constrained optimization problem. We first provide a training function that performs one gradient update per query. There are three types of pairwise fairness criterion we handle (specified by 'constraint_type'), and assign the (pos_group, neg_group) pairs to compare accordingly.","metadata":{"id":"16nddoPIrmuj"}},{"cell_type":"code","source":"def train_model(train_set, params):\n  # Trains the model with stochastic updates (one query per updates).\n  #\n  # Args:\n  #   train_set: Dictionary of \"paired\" training data.\n  #   params: Dictionary of hyper-paramters for training.\n  #\n  # Returns:\n  #   Trained model, list of objectives, list of group constraint violations.\n\n  # Set up problem and model.\n  if params['constrained']:\n    # Constrained optimization.\n    if params['constraint_type'] == 'marginal_equal_opportunity':\n      constraint_groups = [((0, None), (1, None))]\n    elif params['constraint_type'] == 'cross_group_equal_opportunity':\n      constraint_groups = [((0, 1), (1, 0))]\n    else:\n      constraint_groups = [((0, 1), (1, 0)), ((0, 0), (1, 1))]\n  else:\n    # Unconstrained optimization.\n    constraint_groups = []\n\n  # Dictionary that will hold batch features pairs, group pairs and labels for \n  # current batch. We include one query per-batch. \n  paired_batch = {}\n  batch_index = 0  # Index of current query.\n\n  # Data functions.\n  features = lambda: paired_batch['features']\n  groups = lambda: paired_batch['groups'] \n\n  # Create ranking model and constrained optimization problem.\n  problem, ranking_model = formulate_problem(\n      features, groups, train_set['dimension'], constraint_groups, \n      params['constraint_slack'])\n  \n  # Create a loss function for the problem.\n  lagrangian_loss, update_ops, multipliers_variables = (\n      tfco.create_lagrangian_loss(problem, dual_scale=params['dual_scale']))\n\n  # Create optimizer\n  optimizer = tf.keras.optimizers.Adagrad(learning_rate=params['learning_rate'])\n  \n  # List of trainable variables.\n  var_list = (\n      ranking_model.trainable_weights + list(problem.trainable_variables)) # + \n      #[multipliers_variables])\n  \n  # List of objectives, group constraint violations.\n  # violations, and snapshot of models during course of training.\n  objectives = []\n  group_violations = []\n  models = []\n\n  features = train_set['features']\n  queries = train_set['queries']\n  groups = train_set['groups']\n\n  print()\n  # Run loops * iterations_per_loop full batch iterations.\n  for ii in range(params['loops']):\n    for jj in range(params['iterations_per_loop']):\n      # Populate paired_batch dict with all pairs for current query. The batch\n      # index is the same as the current query index.\n      paired_batch = {\n          'features': features[queries == batch_index],\n          'groups': groups[queries == batch_index]\n      }\n\n      # Optimize loss.\n      update_ops()\n      optimizer.minimize(lagrangian_loss, var_list=var_list)\n\n      # Update batch_index, and cycle back once last query is reached.\n      batch_index = (batch_index + 1) % train_set['num_queries']\n    \n    # Snap shot current model.\n    model_copy = tf.keras.models.clone_model(ranking_model)\n    model_copy.set_weights(ranking_model.get_weights())\n    models.append(model_copy)\n\n    # Evaluate metrics for snapshotted model. \n    error, gerr, group_viol = evaluate_results(\n        ranking_model, train_set, params)\n    objectives.append(error)\n    group_violations.append(\n        [x - params['constraint_slack'] for x in group_viol])\n\n    sys.stdout.write(\n        '\\r Loop %d: error = %.3f, max constraint violation = %.3f' % \n        (ii, objectives[-1], max(group_violations[-1])))\n  print()\n  \n  if params['constrained']:\n    # Find model iterate that trades-off between objective and group violations.\n    best_index = tfco.find_best_candidate_index(\n        np.array(objectives), np.array(group_violations), rank_objectives=False)\n  else:\n    # Find model iterate that achieves lowest objective.\n    best_index = np.argmin(objectives)\n\n  return models[best_index]","metadata":{"id":"Md5pDHyBRN83","execution":{"iopub.status.busy":"2022-11-07T00:42:26.734149Z","iopub.execute_input":"2022-11-07T00:42:26.734374Z","iopub.status.idle":"2022-11-07T00:42:26.752828Z","shell.execute_reply.started":"2022-11-07T00:42:26.734349Z","shell.execute_reply":"2022-11-07T00:42:26.751870Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Summarize and Plot Results","metadata":{"id":"WxFJV0tvKvyR"}},{"cell_type":"markdown","source":"Having trained a model, we will need functions to summarize the various evaluation metrics.","metadata":{"id":"i7In7Ra7M_S7"}},{"cell_type":"code","source":"def evaluate_results(model, test_set, params):\n  # Returns overall, group error rates, group-level constraint violations.\n  if params['constraint_type'] == 'marginal_equal_opportunity':\n    g0_error = group_error_rate(model, test_set, 0)\n    g1_error = group_error_rate(model, test_set, 1)\n    group_violations = [g0_error - g1_error, g1_error - g0_error]\n    return (error_rate(model, test_set), [g0_error, g1_error], \n            group_violations)\n  else:\n    g00_error = group_error_rate(model, test_set, 0, 0)\n    g01_error = group_error_rate(model, test_set, 0, 1)\n    g10_error = group_error_rate(model, test_set, 1, 1)\n    g11_error = group_error_rate(model, test_set, 1, 1)\n    group_violations_offdiag = [g01_error - g10_error, g10_error - g01_error]\n    group_violations_diag = [g00_error - g11_error, g11_error - g00_error]\n\n    if params['constraint_type'] == 'cross_group_equal_opportunity':\n      return (error_rate(model, test_set), \n              [[g00_error, g01_error], [g10_error, g11_error]], \n              group_violations_offdiag)\n    else:\n      return (error_rate(model, test_set), \n              [[g00_error, g01_error], [g10_error, g11_error]], \n              group_violations_offdiag + group_violations_diag)\n    \n\ndef display_results(\n    model, test_set, params, method, error_type, show_header=False):\n  # Prints evaluation results for model on test data.\n  error, group_error, diffs = evaluate_results(model, test_set, params)\n\n  if params['constraint_type'] == 'marginal_equal_opportunity':\n    if show_header:\n      print('\\nMethod\\t\\t\\tError\\t\\tOverall\\t\\tGroup 0\\t\\tGroup 1\\t\\tDiff')\n    print('%s\\t%s\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f' % (\n        method, error_type, error, group_error[0], group_error[1], \n        np.max(diffs)))\n  elif params['constraint_type'] == 'cross_group_equal_opportunity':\n    if show_header:\n      print('\\nMethod\\t\\t\\tError\\t\\tOverall\\t\\tGroup 0/1\\tGroup 1/0\\tDiff')\n    print('%s\\t%s\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f' % (\n        method, error_type, error, group_error[0][1], group_error[1][0], \n        np.max(diffs)))\n  else:\n    if show_header:\n      print('\\nMethod\\t\\t\\tError\\t\\tOverall\\t\\tGroup 0/1\\tGroup 1/0\\t' +\n            'Group 0/0\\tGroup 1/1\\tDiff')\n    print('%s\\t%s\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f' % (\n        method, error_type, error, group_error[0][1], group_error[1][0], \n        group_error[0][0], group_error[1][1], np.max(diffs)))","metadata":{"id":"CBl5KfEOPApl","execution":{"iopub.status.busy":"2022-11-07T00:42:26.755920Z","iopub.execute_input":"2022-11-07T00:42:26.757042Z","iopub.status.idle":"2022-11-07T00:42:26.772971Z","shell.execute_reply.started":"2022-11-07T00:42:26.756973Z","shell.execute_reply":"2022-11-07T00:42:26.771918Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Experimental Results","metadata":{"id":"PQR0nnORRedG"}},{"cell_type":"markdown","source":"We now run experiments with two types of pairwise fairness criteria: (1) marginal_equal_opportunity and (2) pairwise equal opportunity. In each case, we compare an unconstrained model trained to optimize the error rate and a constrained model trained with pairwise fairness constraints.\n","metadata":{"id":"jTYOW_EOsrWV"}},{"cell_type":"code","source":"# Convert train/test set to paired data for later evaluation.\npaired_train_set = convert_labeled_to_paired_data(train_set)\npaired_test_set = convert_labeled_to_paired_data(test_set)","metadata":{"id":"qMsrOfh7zkbE","execution":{"iopub.status.busy":"2022-11-07T00:42:26.774544Z","iopub.execute_input":"2022-11-07T00:42:26.774899Z","iopub.status.idle":"2022-11-07T00:42:27.062158Z","shell.execute_reply.started":"2022-11-07T00:42:26.774865Z","shell.execute_reply":"2022-11-07T00:42:27.060440Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"paired_train_set\npaired_test_set","metadata":{"execution":{"iopub.status.busy":"2022-11-07T00:42:27.064909Z","iopub.execute_input":"2022-11-07T00:42:27.065342Z","iopub.status.idle":"2022-11-07T00:42:27.080204Z","shell.execute_reply.started":"2022-11-07T00:42:27.065305Z","shell.execute_reply":"2022-11-07T00:42:27.078562Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'features': array([[[0.09, 0.49, 0.1 , ..., 1.  , 1.  , 1.  ],\n         [0.03, 0.46, 0.04, ..., 1.  , 1.  , 1.  ]],\n \n        [[0.09, 0.49, 0.1 , ..., 1.  , 1.  , 1.  ],\n         [0.06, 0.45, 0.02, ..., 1.  , 1.  , 1.  ]],\n \n        [[0.09, 0.49, 0.1 , ..., 1.  , 1.  , 1.  ],\n         [0.02, 0.59, 0.14, ..., 1.  , 1.  , 1.  ]],\n \n        ...,\n \n        [[0.02, 0.38, 0.41, ..., 1.  , 1.  , 1.  ],\n         [0.01, 0.38, 0.11, ..., 1.  , 1.  , 1.  ]],\n \n        [[0.02, 0.38, 0.41, ..., 1.  , 1.  , 1.  ],\n         [0.02, 0.35, 0.21, ..., 1.  , 1.  , 1.  ]],\n \n        [[0.02, 0.38, 0.41, ..., 1.  , 1.  , 1.  ],\n         [0.03, 0.39, 0.12, ..., 1.  , 1.  , 1.  ]]], dtype=float32),\n 'groups': array([[0., 0.],\n        [0., 0.],\n        [0., 0.],\n        ...,\n        [1., 0.],\n        [1., 1.],\n        [1., 0.]], dtype=float32),\n 'queries': array([0, 0, 0, ..., 9, 9, 9]),\n 'dimension': 145,\n 'num_queries': 10}"},"metadata":{}}]},{"cell_type":"markdown","source":"\n## (1) Marginal Equal Opportunity\n\n\nFor a ranking model $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, recall:\n- $err(f)$ as the pairwise ranking error for model $f$ over all pairs of positive and negative documents\n$$\nerr(f) ~=~ \\mathbf{E}\\big[\\mathbb{I}\\big(f(x) < f(x')\\big) \\,\\big|\\, y = 1,~ y' = 0\\big]\n$$\n\nand we additionally define:\n\n- $err_i(f)$ as the row-marginal pairwise error over positive-negative document pairs where the pos. document is from group $i$, and the neg. document is from either groups\n\n$$\nerr_i(f) = \\mathbf{E}\\big[\\mathbb{I}\\big(f(x) < f(x')\\big) \\,\\big|\\, y = 1,~ y' = 0,~ grp(x) = i\\big]\n$$\n\nThe constrained optimization problem we solve constraints the row-marginal pairwise errors to be similar:\n\n$$min_f\\;err(f)$$\n\n$$\\text{s.t.   }\\;|err_0(f) - err_1(f)| \\leq 0.05$$\n","metadata":{"id":"jqxzaPTEwEIn"}},{"cell_type":"code","source":"# Model hyper-parameters.\nmodel_params = {\n    'loops': 10, \n    'iterations_per_loop': 250, \n    'learning_rate': 0.1,\n    'constraint_type': 'marginal_equal_opportunity', \n    'constraint_slack': 0.05,\n    'dual_scale': 0.1}\n\n# Unconstrained optimization.\nmodel_params['constrained'] = False\nmodel_unc  = train_model(paired_train_set, model_params)\ndisplay_results(model_unc, paired_train_set, model_params, 'Unconstrained     ', \n                'Train', show_header=True)\ndisplay_results(model_unc, paired_test_set, model_params,  'Unconstrained     ', \n                'Test')\n\n# Constrained optimization with TFCO.\nmodel_params['constrained'] = True\nmodel_con  = train_model(paired_train_set, model_params)\ndisplay_results(model_con, paired_train_set, model_params, 'Constrained     ', \n                'Train', show_header=True)\ndisplay_results(model_con, paired_test_set, model_params, 'Constrained     ', \n                'Test')","metadata":{"id":"1JsuylHbjrBX","outputId":"cbf0f2a0-5cd8-44af-c1f7-05d2b9f1a72b","execution":{"iopub.status.busy":"2022-11-07T01:05:09.954742Z","iopub.execute_input":"2022-11-07T01:05:09.955152Z","iopub.status.idle":"2022-11-07T01:08:38.724592Z","shell.execute_reply.started":"2022-11-07T01:05:09.955117Z","shell.execute_reply":"2022-11-07T01:08:38.722780Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\n Loop 9: error = 0.057, max constraint violation = 0.009\n\nMethod\t\t\tError\t\tOverall\t\tGroup 0\t\tGroup 1\t\tDiff\nUnconstrained     \tTrain\t\t0.057\t\t0.095\t\t0.036\t\t0.059\nUnconstrained     \tTest\t\t0.079\t\t0.150\t\t0.043\t\t0.107\n\n Loop 9: error = 0.057, max constraint violation = 0.009\n\nMethod\t\t\tError\t\tOverall\t\tGroup 0\t\tGroup 1\t\tDiff\nConstrained     \tTrain\t\t0.057\t\t0.095\t\t0.036\t\t0.059\nConstrained     \tTest\t\t0.079\t\t0.150\t\t0.043\t\t0.107\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## (2) Pairwise Equal Opportunity\n\nRecall that we denote\n $err_{i,j}(f)$ as the ranking error over positive-negative document pairs where the pos. document is from group $i$, and the neg. document is from group $j$.\n$$\nerr_{i, j}(f) ~=~ \\mathbf{E}\\big[\\mathbb{I}\\big(f(x) < f(x')\\big) \\,\\big|\\, y = 1,~ y' = 0,~ grp(x) = i, ~grp(x') = j\\big]\n$$\n\n\nWe first constrain only the cross-group errors, highlighted below.\n\n<br>\n<table border='1' bordercolor='black'>\n  <tr >\n     <td bgcolor='white'> </td>\n     <td bgcolor='white'> </td>\n     <td bgcolor='white'  colspan=2 align=center><b>Negative</b></td>\n  </tr>\n  <tr>\n    <td bgcolor='white'></td>\n    <td bgcolor='white'></td>\n    <td>Group 0</td>\n    <td>Group 1</td>\n  </tr>\n  <tr>\n    <td bgcolor='white' rowspan=2><b>Positive</b></td>\n    <td bgcolor='white'>Group 0</td>\n    <td bgcolor='white'>$err_{0,0}$</td>\n    <td bgcolor='white'>$\\mathbf{err_{0,1}}$</td>\n  </tr>\n  <tr>\n    <td>Group 1</td>\n     <td bgcolor='white'>$\\mathbf{err_{1,0}}$</td>\n      <td bgcolor='white'>$err_{1,1}$</td>\n  </tr>\n</table>\n<br>\n\nThe optimization problem we solve constraints the cross-group pairwise errors to be similar:\n\n$$min_f\\; err(f)$$\n$$\\text{s.t. }\\;\\; |err_{0,1}(f) - err_{1,0}(f)| \\leq 0.05$$\n","metadata":{"id":"CorY2URkop1Y"}},{"cell_type":"code","source":"# Model hyper-parameters.\nmodel_params = {\n    'loops': 10, \n    'iterations_per_loop': 250, \n    'learning_rate': 0.1,\n    'constraint_type': 'cross_group_equal_opportunity', \n    'constraint_slack': 0.05,\n    'dual_scale': 0.1}\n\n# Unconstrained optimization.\nmodel_params['constrained'] = False\nmodel_unc  = train_model(paired_train_set, model_params)\ndisplay_results(model_unc, paired_train_set, model_params, 'Unconstrained     ', \n                'Train', show_header=True)\ndisplay_results(model_unc, paired_test_set, model_params,  'Unconstrained     ', \n                'Test')\n\n# Constrained optimization with TFCO.\nmodel_params['constrained'] = True\nmodel_con  = train_model(paired_train_set, model_params)\ndisplay_results(model_con, paired_train_set, model_params, 'Constrained     ', \n                'Train', show_header=True)\ndisplay_results(model_con, paired_test_set, model_params, 'Constrained     ', \n                'Test')","metadata":{"id":"vqat8pXHStjw","outputId":"b8d45ebf-9d98-47a3-9ced-4c1acee31307","execution":{"iopub.status.busy":"2022-11-07T00:58:47.963418Z","iopub.execute_input":"2022-11-07T00:58:47.963750Z","iopub.status.idle":"2022-11-07T01:03:14.294353Z","shell.execute_reply.started":"2022-11-07T00:58:47.963726Z","shell.execute_reply":"2022-11-07T01:03:14.293034Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n Loop 9: error = 0.057, max constraint violation = 0.118\n\nMethod\t\t\tError\t\tOverall\t\tGroup 0/1\tGroup 1/0\tDiff\nUnconstrained     \tTrain\t\t0.057\t\t0.296\t\t0.128\t\t0.168\nUnconstrained     \tTest\t\t0.079\t\t0.340\t\t0.135\t\t0.205\n\n Loop 9: error = 0.057, max constraint violation = 0.118\n\nMethod\t\t\tError\t\tOverall\t\tGroup 0/1\tGroup 1/0\tDiff\nConstrained     \tTrain\t\t0.057\t\t0.296\t\t0.129\t\t0.167\nConstrained     \tTest\t\t0.078\t\t0.335\t\t0.133\t\t0.202\n","output_type":"stream"}]}]}